{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Chain of Thought (CoT) Creation\n",
    "\n",
    "In this notebook, we aim to create a Chain of Thought (CoT) for clinical trial studies using HuggingFace's transformers and other relevant libraries. Below is a step-by-step guide to our workflow:\n",
    "\n",
    "1. **Setup and Initialization**:\n",
    "  - We initialize the ChromaDB client and load the SentenceTransformer model for encoding queries.\n",
    "  - We define a function to fix invalid JSON strings, which is crucial for handling the metadata of clinical trial studies.\n",
    "\n",
    "2. **Retrieving Relevant Studies**:\n",
    "  - We define a function `retrieve_relevant_studies` that queries the ChromaDB collection to find studies relevant to a given query, excluding the study already present in the query.\n",
    "\n",
    "3. **Crafting Context from Studies**:\n",
    "  - We define a function `craft_context_from_studies_documents` to create a context string from the documents of related studies. This context is used to provide examples in the CoT creation process.\n",
    "\n",
    "4. **Generating Messages for CoT**:\n",
    "  - We define a function `get_messages_for_create_CoT` that generates the system and user messages required for creating a CoT. These messages include the study title, description, and desired criteria.\n",
    "\n",
    "5. **Prompt Creation**:\n",
    "  - We define a function `get_prompt_from_studies` that uses the above functions to generate the complete prompt for a given study. This prompt includes the context from related studies and the task instructions for generating the CoT.\n",
    "\n",
    "6. **Model Inference**:\n",
    "  - We load the HuggingFace model and tokenizer, and define a function `pipe` to generate the CoT using the model. The function takes the generated messages as input and returns the model's output.\n",
    "  - For Gemini, can use the function in the gemini section to generate the CoT.\n",
    "\n",
    "By following this workflow, we can systematically generate a Chain of Thought for clinical trial studies, leveraging the capabilities of HuggingFace's transformers and other relevant tools.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "client = chromadb.PersistentClient(path=\"./clinical_trials_chroma\")\n",
    "embed_model = SentenceTransformer(\"malteos/scincl\")\n",
    "collection = client.get_or_create_collection(\"clinical_trials_studies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "# def fix_invalid_json(input_str):\n",
    "#     ## add double quotes around elements inside square brackets if not already quoted\n",
    "#     fixed_str = re.sub(r'(?<=\\[)([^\\[\\],]+)(?=\\])', lambda x: '\"' + x.group(0).strip() + '\"', input_str)\n",
    "    \n",
    "#     ## add double quotes around words in Conditions and Interventions\n",
    "#     fixed_str = re.sub(r'(?<=\\[)([^\\\"\\]]+?)(?=\\])', lambda x: '\"' + x.group(0).strip().replace(\", \", '\", \"') + '\"', fixed_str)\n",
    "    \n",
    "#     ## fix key-value pairs inside Interventions\n",
    "#     fixed_str = re.sub(r'\"([A-Za-z]+): ([A-Za-z0-9\\s]+)\"', r'\"\\1: \\2\"', fixed_str)\n",
    "    \n",
    "#     # fix dictionary keys\n",
    "#     fixed_str = re.sub(r'(?<!\")(\\b[A-Za-z_]+\\b)(?=\\s*:)', r'\"\\1\"', fixed_str)\n",
    "    \n",
    "#     return fixed_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve relevent studies from chromadb but exclude the ones that are already is the query\n",
    "def retrieve_relevant_studies(query, existing_study, n_results=5):\n",
    "    query_embedding = embed_model.encode(query).tolist()\n",
    "    \n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_embedding],\n",
    "        n_results=n_results + 1,\n",
    "    )\n",
    "    \n",
    "    filtered_results = []\n",
    "    for id, distance, document in zip(results['ids'][0], results['distances'][0], results['documents'][0]):\n",
    "        if id != existing_study:\n",
    "            filtered_results.append({\n",
    "                \"id\": id,\n",
    "                \"distance\": distance,\n",
    "                \"document\": document,\n",
    "            })\n",
    "        \n",
    "        if len(filtered_results) == n_results:\n",
    "            break\n",
    "    \n",
    "    return filtered_results\n",
    "\n",
    "print(\n",
    "    retrieve_relevant_studies(\"Effect of Kinesiotaping on Edema Management, Pain and Function on Patients With Bilateral Total Knee Arthroplasty [SEP] After being informed about the study and potential risk, all patients undergoing inpatient rehabilitation after bilateral total knee arthroplasty will have Kinesio(R)Tape applied to one randomly selected leg while the other leg serves as a control. Measurement of bilateral leg circumference, knee range of motion, numerical rating scale for pain, and selected questions from the Knee Injury and Osteoarthritis Outcome Score will occur at regular intervals throughout the rehabilitation stay. Patients will receive standard rehabilitation.\", \n",
    "                              \"NCT05013879\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import json_repair\n",
    "def related_studies_template(title: str, description: str, criteria: str):\n",
    "    return f\"\"\"Example Title: {title}\n",
    "Example Description: {description}\n",
    "Example Criteria: {criteria}\n",
    "\"\"\"\n",
    "\n",
    "def craft_context_from_studies_documents(related_studies: list[str]):\n",
    "    json_related_studies = [json.loads(i) for i in related_studies]\n",
    "    context = \"\"\n",
    "    for i in json_related_studies:\n",
    "        title = i.get('metadata', {}).get('Official_title', \"\")\n",
    "        description = i.get('description', \"\")\n",
    "        criteria = i.get('criteria', \"\")\n",
    "        if title and description:\n",
    "            context += f\"\"\"<STUDY>\n",
    "{related_studies_template(title, description, criteria)}\n",
    "</STUDY>\"\"\"\n",
    "    return context\n",
    "\n",
    "def user_prompt_template(encoded_related_studies: str, title: str, description: str, desired_criteria: str):\n",
    "    user_prompt_template = \"\"\"<EXAMPLE_STUDIES>{encoded_related_studies}</EXAMPLE_STUDIES>\n",
    "\n",
    "Title: {title}\n",
    "Description: {description}\n",
    "Desired criteria: {desired_criteria}\n",
    "\n",
    "Task Instructions:\n",
    "1. Derive a step-by-step justification starting from the \"Title\" and \"Description\" provided, gradually building up to support the \"Desired criteria\".\n",
    "2. Clearly explain the rationale behind each parameter of all criteria, including values, thresholds, and other specific details.\n",
    "3. Could use example studies (in the <EXAMPLE_STUDIES> section) if they support your justifications, but ensure the reasoning is well-explained and relevant to the study's context.\n",
    "4. Avoid mentioning that the criteria were already provided, and please do not cite the \"Desired criteria\" directly in your justification.\n",
    "5. You should give the justification first before giving out the criteria.\n",
    "\n",
    "Response Format:\n",
    "<STEP-BY-STEP-JUSTIFICATION>\n",
    "Your long step by step detailed logical justification here.\n",
    "</STEP-BY-STEP-JUSTIFICATION>\n",
    "\"\"\"\n",
    "\n",
    "    return user_prompt_template.format(encoded_related_studies=encoded_related_studies, title=title, description=description, desired_criteria=desired_criteria)\n",
    "\n",
    "system_prompt = \"You are a justifier chatbot designed to generate step-by-step justifications that derived form the Title and Description of a study and then gradually build up to the Desired criteria. Your task is to analyze the title and description of a study and build logical, step-by-step justifications that connect the study’s key elements to the desired criteria. Reference related example studies if they reinforce your justifications. You must assume the desired criteria are correct (as it was already reviewed by specialists) and develop arguments to support them based on the study context and relevant research insights.\"\n",
    "\n",
    "def get_messages_for_CoT_huggingface(encoded_related_studies: str, title: str, description: str, desired_criteria: str):\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": \"You are a justifier chatbot designed to generate step-by-step justifications that derived form the Title and Description of a study and then gradually build up to the Desired criteria. Your task is to analyze the title and description of a study and build logical, step-by-step justifications that connect the study’s key elements to the desired criteria. Reference related example studies if they reinforce your justifications. You must assume the desired criteria are correct (as it was already reviewed by specialists) and develop arguments to support them based on the study context and relevant research insights.\"},\n",
    "        {\"role\": \"user\", \"content\": user_prompt_template(encoded_related_studies, title, description, desired_criteria)},\n",
    "    ]\n",
    "    \n",
    "\n",
    "def get_info_for_prompt_gen(study_info: dict):\n",
    "    metadata = json_repair.loads(study_info.get('metadata'))\n",
    "    title = metadata.get('Official_title')\n",
    "    description = study_info.get('data')\n",
    "    study_id = metadata.get('NCT_ID')\n",
    "    desired_criteria = study_info.get('criteria')\n",
    "\n",
    "    # Ensure we have the minimum required information\n",
    "    if not title or not description or not desired_criteria or not study_id:\n",
    "        print(f\"Skipping study {study_id}: Missing title or description or desired criteria or study id\")\n",
    "        return None\n",
    "\n",
    "    query = f'{title} [SEP] {description}'\n",
    "    relevant_studies = retrieve_relevant_studies(query, study_id)\n",
    "    encoded_related_studies = craft_context_from_studies_documents([i['document'] for i in relevant_studies])\n",
    "    return encoded_related_studies, title, description, desired_criteria\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Llama 3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_id = \"neuralmagic/Meta-Llama-3.1-8B-Instruct-quantized.w8a16\"\n",
    "number_gpus = 1\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "llm = LLM(model=model_id, tensor_parallel_size=number_gpus, max_model_len=20000)\n",
    "\n",
    "def pipe(messages):\n",
    "    sampling_params = SamplingParams(temperature=0, top_p=0.9, max_tokens=4096)\n",
    "    prompts = tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n",
    "    outputs = llm.generate(prompts, sampling_params)\n",
    "    return [i.outputs[0].text for i in outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load the JSON data\n",
    "ravis_dataset = load_dataset(\"ravistech/clinical-trial-llm-cancer-restructure\")\n",
    "\n",
    "# Initialize an empty DataFrame\n",
    "df = pd.DataFrame(columns=['encoded_related_studies', 'title', 'description', 'desired_criteria', 'messages'])\n",
    "\n",
    "for study in tqdm(ravis_dataset['train']):\n",
    "    print(f\"Processing {study}\")\n",
    "    info_for_prompt = get_info_for_prompt_gen(study)\n",
    "    \n",
    "    if info_for_prompt:\n",
    "        encoded_related_studies, title, description, desired_criteria = info_for_prompt\n",
    "        messages = get_messages_for_CoT_huggingface(encoded_related_studies, title, description, desired_criteria)\n",
    "        \n",
    "        # Add the data to the DataFrame\n",
    "        df = df.append({\n",
    "            'encoded_related_studies': encoded_related_studies,\n",
    "            'title': title,\n",
    "            'description': description,\n",
    "            'desired_criteria': desired_criteria,\n",
    "            'messages': messages\n",
    "        }, ignore_index=True)\n",
    "        \n",
    "        print(f\"Prompt: {messages}\")\n",
    "        print(f\"Response: {pipe(messages)}\")\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('responses_gemini.csv', index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import json\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import vertexai\n",
    "from vertexai.batch_prediction import BatchPredictionJob\n",
    "import os\n",
    "import uuid\n",
    "\n",
    "PROJECT_ID = \"PROJECT-ID\"  # update with Google Cloud project ID\n",
    "BUCKET_NAME = \"BUCKET-NAME\" # create a bucket in Google Cloud Storage first\n",
    "OUTPUT_DIR = \"./output/\"\n",
    "CSV_OUTPUT_PATH = \"./responses_gemini.csv\"\n",
    "vertexai.init(project=PROJECT_ID, location=\"us-central1\")\n",
    "\n",
    "ravis_dataset = load_dataset(\"ravistech/clinical-trial-llm-cancer-restructure\")\n",
    "\n",
    "batch_prompts = []\n",
    "info_data = {}  # key is uuid, value is the info for the prompt\n",
    "for study in tqdm(ravis_dataset['train']):\n",
    "    info_for_prompt = get_info_for_prompt_gen(study)\n",
    "    \n",
    "    if info_for_prompt:\n",
    "        unique_id = str(uuid.uuid4())  # gen uuid for each entries\n",
    "        encoded_related_studies, title, description, desired_criteria = info_for_prompt\n",
    "        messages = user_prompt_template(encoded_related_studies, title, description, desired_criteria)\n",
    "\n",
    "        request_format = {\n",
    "            \"id\": unique_id,\n",
    "            \"request\": {\n",
    "                \"contents\": [\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"parts\": [\n",
    "                            {\"text\": messages}\n",
    "                        ]\n",
    "                    }\n",
    "                ],\n",
    "                \"system_instruction\": {\n",
    "                    \"parts\": [\n",
    "                        {\"text\": system_prompt}\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        batch_prompts.append(request_format)\n",
    "        \n",
    "        # store info with unique ID for later matching with the response\n",
    "        info_data[unique_id] = {\n",
    "            \"encoded_related_studies\": encoded_related_studies,\n",
    "            \"title\": title,\n",
    "            \"description\": description,\n",
    "            \"desired_criteria\": desired_criteria,\n",
    "            \"messages\": messages,\n",
    "            \"response\": None\n",
    "        }\n",
    "\n",
    "input_jsonl_path = f'gs://{BUCKET_NAME}/prompt_for_batch_gemini_predict.jsonl'\n",
    "with open('./prompt_for_batch_gemini_predict.jsonl', 'w') as f:\n",
    "    for prompt in batch_prompts:\n",
    "        f.write(json.dumps(prompt) + '\\n')\n",
    "\n",
    "# Upload the JSONL file to GCS\n",
    "!gsutil cp ./prompt_for_batch_gemini_predict.jsonl {input_jsonl_path}\n",
    "\n",
    "batch_prediction_job = BatchPredictionJob.submit(\n",
    "    source_model=\"gemini-1.5-flash-002\",\n",
    "    input_dataset=input_jsonl_path,\n",
    "    output_uri_prefix=f'gs://{BUCKET_NAME}/output/',\n",
    ")\n",
    "\n",
    "print(f\"Job resource name: {batch_prediction_job.resource_name}\")\n",
    "print(f\"Model resource name with the job: {batch_prediction_job.model_name}\")\n",
    "print(f\"Job state: {batch_prediction_job.state.name}\")\n",
    "\n",
    "# Check the job status\n",
    "while not batch_prediction_job.has_ended:\n",
    "    time.sleep(5)\n",
    "    batch_prediction_job.refresh()\n",
    "\n",
    "# Process the output if the job is successful\n",
    "if batch_prediction_job.has_succeeded:\n",
    "    print(\"Job succeeded!\")\n",
    "    output_location = batch_prediction_job.output_location + \"/predictions.jsonl\"\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    local_output_path = os.path.join(OUTPUT_DIR, \"predictions.jsonl\")\n",
    "\n",
    "    # Download the output file from GCS\n",
    "    !gsutil cp {output_location} {local_output_path}\n",
    "    print(f\"Output file downloaded to: {local_output_path}\")\n",
    "    \n",
    "    with open(local_output_path, 'r') as f:\n",
    "        for line in f:\n",
    "            response_data = json.loads(line)\n",
    "            unique_id = response_data.get(\"id\")\n",
    "            response = response_data.get(\"response\", {})\n",
    "            \n",
    "            if unique_id in info_data:\n",
    "                info_data[unique_id][\"response\"] = response\n",
    "\n",
    "    df = pd.DataFrame.from_dict(info_data, orient=\"index\")\n",
    "    df.to_csv(CSV_OUTPUT_PATH, index=False)\n",
    "    print(f\"Responses saved to CSV at: {CSV_OUTPUT_PATH}\")\n",
    "else:\n",
    "    print(f\"Job failed: {batch_prediction_job.error}\")\n",
    "\n",
    "print(f\"Job output location: {batch_prediction_job.output_location}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
