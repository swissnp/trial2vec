{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Chain of Thought (CoT) Creation\n",
    "\n",
    "In this notebook, we aim to create a Chain of Thought (CoT) for clinical trial studies using HuggingFace's transformers and other relevant libraries. Below is a step-by-step guide to our workflow:\n",
    "\n",
    "1. **Setup and Initialization**:\n",
    "  - We initialize the ChromaDB client and load the SentenceTransformer model for encoding queries.\n",
    "  - We define a function to fix invalid JSON strings, which is crucial for handling the metadata of clinical trial studies.\n",
    "\n",
    "2. **Retrieving Relevant Studies**:\n",
    "  - We define a function `retrieve_relevant_studies` that queries the ChromaDB collection to find studies relevant to a given query, excluding the study already present in the query.\n",
    "\n",
    "3. **Crafting Context from Studies**:\n",
    "  - We define a function `craft_context_from_studies_documents` to create a context string from the documents of related studies. This context is used to provide examples in the CoT creation process.\n",
    "\n",
    "4. **Generating Messages for CoT**:\n",
    "  - We define a function `get_messages_for_create_CoT` that generates the system and user messages required for creating a CoT. These messages include the study title, description, and desired criteria.\n",
    "\n",
    "5. **Prompt Creation**:\n",
    "  - We define a function `get_prompt_from_studies` that uses the above functions to generate the complete prompt for a given study. This prompt includes the context from related studies and the task instructions for generating the CoT.\n",
    "\n",
    "6. **Model Inference**:\n",
    "  - We load the HuggingFace model and tokenizer, and define a function `pipe` to generate the CoT using the model. The function takes the generated messages as input and returns the model's output.\n",
    "  - For Gemini, can use the function in the gemini section to generate the CoT.\n",
    "\n",
    "By following this workflow, we can systematically generate a Chain of Thought for clinical trial studies, leveraging the capabilities of HuggingFace's transformers and other relevant tools.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "client = chromadb.PersistentClient(path=\"./clinical_trials_chroma\")\n",
    "embed_model = SentenceTransformer(\"malteos/scincl\")\n",
    "collection = client.get_or_create_collection(\"clinical_trials_studies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def fix_invalid_json(input_str):\n",
    "    ## add double quotes around elements inside square brackets if not already quoted\n",
    "    fixed_str = re.sub(r'(?<=\\[)([^\\[\\],]+)(?=\\])', lambda x: '\"' + x.group(0).strip() + '\"', input_str)\n",
    "    \n",
    "    ## add double quotes around words in Conditions and Interventions\n",
    "    fixed_str = re.sub(r'(?<=\\[)([^\\\"\\]]+?)(?=\\])', lambda x: '\"' + x.group(0).strip().replace(\", \", '\", \"') + '\"', fixed_str)\n",
    "    \n",
    "    ## fix key-value pairs inside Interventions\n",
    "    fixed_str = re.sub(r'\"([A-Za-z]+): ([A-Za-z0-9\\s]+)\"', r'\"\\1: \\2\"', fixed_str)\n",
    "    \n",
    "    # fix dictionary keys\n",
    "    fixed_str = re.sub(r'(?<!\")(\\b[A-Za-z_]+\\b)(?=\\s*:)', r'\"\\1\"', fixed_str)\n",
    "    \n",
    "    return fixed_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve relevent studies from chromadb but exclude the ones that are already is the query\n",
    "def retrieve_relevant_studies(query, existing_study, n_results=5):\n",
    "    query_embedding = embed_model.encode(query).tolist()\n",
    "    \n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_embedding],\n",
    "        n_results=n_results + 1,\n",
    "    )\n",
    "    \n",
    "    filtered_results = []\n",
    "    for id, distance, document in zip(results['ids'][0], results['distances'][0], results['documents'][0]):\n",
    "        if id != existing_study:\n",
    "            filtered_results.append({\n",
    "                \"id\": id,\n",
    "                \"distance\": distance,\n",
    "                \"document\": document,\n",
    "            })\n",
    "        \n",
    "        if len(filtered_results) == n_results:\n",
    "            break\n",
    "    \n",
    "    return filtered_results\n",
    "\n",
    "print(\n",
    "    retrieve_relevant_studies(\"Effect of Kinesiotaping on Edema Management, Pain and Function on Patients With Bilateral Total Knee Arthroplasty [SEP] After being informed about the study and potential risk, all patients undergoing inpatient rehabilitation after bilateral total knee arthroplasty will have Kinesio(R)Tape applied to one randomly selected leg while the other leg serves as a control. Measurement of bilateral leg circumference, knee range of motion, numerical rating scale for pain, and selected questions from the Knee Injury and Osteoarthritis Outcome Score will occur at regular intervals throughout the rehabilitation stay. Patients will receive standard rehabilitation.\", \n",
    "                              \"NCT05013879\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import json_repair\n",
    "def related_studies_template(title: str, description: str, criteria: str):\n",
    "    return f\"\"\"Example Title: {title}\n",
    "Example Description: {description}\n",
    "Example Criteria: {criteria}\n",
    "\"\"\"\n",
    "\n",
    "def craft_context_from_studies_documents(related_studies: list[str]):\n",
    "    json_related_studies = [json.loads(i) for i in related_studies]\n",
    "    context = \"\"\n",
    "    for i in json_related_studies:\n",
    "        title = i.get('metadata', {}).get('Official_title', \"\")\n",
    "        description = i.get('description', \"\")\n",
    "        criteria = i.get('criteria', \"\")\n",
    "        if title and description:\n",
    "            context += f\"\"\"<STUDY>\n",
    "{related_studies_template(title, description, criteria)}\n",
    "</STUDY>\"\"\"\n",
    "    return context\n",
    "\n",
    "def user_prompt_template(encoded_related_studies: str, title: str, description: str, desired_criteria: str):\n",
    "    user_prompt_template = \"\"\"<EXAMPLE_STUDIES>{encoded_related_studies}</EXAMPLE_STUDIES>\n",
    "\n",
    "Title: {title}\n",
    "Description: {description}\n",
    "Desired criteria: {desired_criteria}\n",
    "\n",
    "Task Instructions:\n",
    "1. Derive a step-by-step justification starting from the Title and Description provided, gradually building up to support the Desired criteria.\n",
    "2. Could use example studies (in the <EXAMPLE_STUDIES> section) if they support your justifications, but ensure the reasoning is well-explained and relevant to the study's context.\n",
    "4. Avoid mentioning that the criteria were already provided, and please do not cite the given criteria directly in your justification.\n",
    "5. You should give the justification first before giving out the criteria.\n",
    "\n",
    "Response Format:\n",
    "<STEP-BY-STEP-JUSTIFICATION>\n",
    "Your step by step justification here.\n",
    "</STEP-BY-STEP-JUSTIFICATION>\n",
    "<Criteria>\n",
    "The copied desired criteria here.\n",
    "</Criteria>\n",
    "\"\"\"\n",
    "\n",
    "    return user_prompt_template.format(encoded_related_studies=encoded_related_studies, title=title, description=description, desired_criteria=desired_criteria)\n",
    "\n",
    "system_prompt = \"You are a justifier chatbot designed to generate step-by-step justifications that derived form the Title and Description of a study and then gradually build up to the Desired criteria. Your task is to analyze the title and description of a study and build logical, step-by-step justifications that connect the study’s key elements to the desired criteria. Reference related example studies if they reinforce your justifications. You must assume the desired criteria are correct (as it was already reviewed by specialists) and develop arguments to support them based on the study context and relevant research insights.\"\n",
    "\n",
    "def get_messages_for_CoT_huggingface(encoded_related_studies: str, title: str, description: str, desired_criteria: str):\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": \"You are a justifier chatbot designed to generate step-by-step justifications that derived form the Title and Description of a study and then gradually build up to the Desired criteria. Your task is to analyze the title and description of a study and build logical, step-by-step justifications that connect the study’s key elements to the desired criteria. Reference related example studies if they reinforce your justifications. You must assume the desired criteria are correct (as it was already reviewed by specialists) and develop arguments to support them based on the study context and relevant research insights.\"},\n",
    "        {\"role\": \"user\", \"content\": user_prompt_template(encoded_related_studies, title, description, desired_criteria)},\n",
    "    ]\n",
    "    \n",
    "\n",
    "def get_info_for_prompt_gen(study_info: dict):\n",
    "    metadata = json_repair.loads(fix_invalid_json(study_info.get('metadata')))\n",
    "    title = metadata.get('Official_title')\n",
    "    description = study_info.get('data')\n",
    "    study_id = metadata.get('NCT_ID')\n",
    "    desired_criteria = study_info.get('criteria')\n",
    "\n",
    "    # Ensure we have the minimum required information\n",
    "    if not title or not description or not desired_criteria or not study_id:\n",
    "        print(f\"Skipping study {study_id}: Missing title or description or desired criteria or study id\")\n",
    "        return None\n",
    "\n",
    "    query = f'{title} [SEP] {description}'\n",
    "    relevant_studies = retrieve_relevant_studies(query, study_id)\n",
    "    encoded_related_studies = craft_context_from_studies_documents([i['document'] for i in relevant_studies])\n",
    "    return encoded_related_studies, title, description, desired_criteria\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Llama 3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_id = \"neuralmagic/Meta-Llama-3.1-8B-Instruct-quantized.w8a16\"\n",
    "number_gpus = 1\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "llm = LLM(model=model_id, tensor_parallel_size=number_gpus, max_model_len=20000)\n",
    "\n",
    "def pipe(messages):\n",
    "    sampling_params = SamplingParams(temperature=0, top_p=0.9, max_tokens=4096)\n",
    "    prompts = tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n",
    "    outputs = llm.generate(prompts, sampling_params)\n",
    "    return [i.outputs[0].text for i in outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "# Load the JSON data\n",
    "\n",
    "ravis_dataset = load_dataset(\"ravistech/clinical-trial-llm-cancer-restructure\")\n",
    "\n",
    "for study in tqdm(ravis_dataset['train']):\n",
    "    print(f\"Processing {study['metadata']}\")\n",
    "    info_for_prompt = get_info_for_prompt_gen(study)\n",
    "    messages = get_messages_for_CoT_huggingface(*info_for_prompt)\n",
    "    print(f\"Prompt: {messages}\")\n",
    "    print(f\"Response: {pipe(messages)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U google-generativeai\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import google.generativeai as genai\n",
    "\n",
    "# Configure the Gemini model\n",
    "genai.configure(api_key=\"YOUR_API_KEY\")\n",
    "model = genai.GenerativeModel(\"gemini-1.5-flash\",\n",
    "                              system_instruction=system_prompt)\n",
    "\n",
    "# Load the JSON data\n",
    "ravis_dataset = load_dataset(\"ravistech/clinical-trial-llm-cancer-restructure\")\n",
    "\n",
    "for study in tqdm(ravis_dataset['train']):\n",
    "    print(f\"Processing {study['metadata']}\")\n",
    "    info_for_prompt = get_info_for_prompt_gen(study)\n",
    "    messages = user_prompt_template(*info_for_prompt)\n",
    "    print(f\"Prompt: {messages}\")\n",
    "    response = model.generate_content(messages)\n",
    "    print(f\"Response: {response.text}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
