{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Chain of Thought (CoT) Creation\n",
    "\n",
    "In this notebook, we aim to create a Chain of Thought (CoT) for clinical trial studies using HuggingFace's transformers and other relevant libraries. Below is a step-by-step guide to our workflow:\n",
    "\n",
    "1. **Setup and Initialization**:\n",
    "  - We initialize the ChromaDB client and load the SentenceTransformer model for encoding queries.\n",
    "  - We define a function to fix invalid JSON strings, which is crucial for handling the metadata of clinical trial studies.\n",
    "\n",
    "2. **Retrieving Relevant Studies**:\n",
    "  - We define a function `retrieve_relevant_studies` that queries the ChromaDB collection to find studies relevant to a given query, excluding the study already present in the query.\n",
    "\n",
    "3. **Crafting Context from Studies**:\n",
    "  - We define a function `craft_context_from_studies_documents` to create a context string from the documents of related studies. This context is used to provide examples in the CoT creation process.\n",
    "\n",
    "4. **Generating Messages for CoT**:\n",
    "  - We define a function `get_messages_for_create_CoT` that generates the system and user messages required for creating a CoT. These messages include the study title, description, and desired criteria.\n",
    "\n",
    "5. **Prompt Creation**:\n",
    "  - We define a function `get_prompt_from_studies` that uses the above functions to generate the complete prompt for a given study. This prompt includes the context from related studies and the task instructions for generating the CoT.\n",
    "\n",
    "6. **Model Inference**:\n",
    "  - We load the HuggingFace model and tokenizer, and define a function `pipe` to generate the CoT using the model. The function takes the generated messages as input and returns the model's output.\n",
    "  - For Gemini, can use the function in the gemini section to generate the CoT.\n",
    "\n",
    "By following this workflow, we can systematically generate a Chain of Thought for clinical trial studies, leveraging the capabilities of HuggingFace's transformers and other relevant tools.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from client import Client\n",
    "from prompt_gen import PromptGen\n",
    "\n",
    "client = chromadb.PersistentClient(path=\"./clinical_trials_chroma\")\n",
    "embed_model = SentenceTransformer(\"malteos/scincl\")\n",
    "collection = client.get_or_create_collection(\"clinical_trials_studies\")\n",
    "\n",
    "\n",
    "client = Client(\n",
    "  client=client,\n",
    "  collection=collection,\n",
    "  embed_model=embed_model\n",
    ")\n",
    "\n",
    "prompt_gen = PromptGen(\n",
    "  client=client\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from client import Client\n",
    "from prompt_gen import PromptGen\n",
    "\n",
    "client = Client(\n",
    "  client=client,\n",
    "  collection=collection,\n",
    "  embed_model=embed_model\n",
    ")\n",
    "\n",
    "prompt_gen = PromptGen(\n",
    "  client=client\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    client.retrieve_relevant_studies(\"Effect of Kinesiotaping on Edema Management, Pain and Function on Patients With Bilateral Total Knee Arthroplasty [SEP] After being informed about the study and potential risk, all patients undergoing inpatient rehabilitation after bilateral total knee arthroplasty will have Kinesio(R)Tape applied to one randomly selected leg while the other leg serves as a control. Measurement of bilateral leg circumference, knee range of motion, numerical rating scale for pain, and selected questions from the Knee Injury and Osteoarthritis Outcome Score will occur at regular intervals throughout the rehabilitation stay. Patients will receive standard rehabilitation.\", \n",
    "                              \"NCT05013879\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Llama 3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_id = \"neuralmagic/Meta-Llama-3.1-8B-Instruct-quantized.w8a16\"\n",
    "number_gpus = 1\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "llm = LLM(model=model_id, tensor_parallel_size=number_gpus, max_model_len=20000)\n",
    "\n",
    "def pipe(messages):\n",
    "    sampling_params = SamplingParams(temperature=0, top_p=0.9, max_tokens=4096)\n",
    "    prompts = tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n",
    "    outputs = llm.generate(prompts, sampling_params)\n",
    "    return [i.outputs[0].text for i in outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load the JSON data\n",
    "ravis_dataset = load_dataset(\"ravistech/clinical-trial-llm-cancer-restructure\")\n",
    "\n",
    "# Initialize an empty DataFrame\n",
    "df = pd.DataFrame(columns=['encoded_related_studies', 'title', 'description', 'desired_criteria', 'messages'])\n",
    "\n",
    "for study in tqdm(ravis_dataset['train']):\n",
    "    print(f\"Processing {study}\")\n",
    "    info_for_prompt = prompt_gen.get_info_for_prompt_gen(study)\n",
    "    \n",
    "    if info_for_prompt:\n",
    "        encoded_related_studies, title, description, desired_criteria = info_for_prompt\n",
    "        messages = prompt_gen.get_messages_for_CoT_huggingface(encoded_related_studies, title, description, desired_criteria)\n",
    "        \n",
    "        # Add the data to the DataFrame\n",
    "        df = df.append({\n",
    "            'encoded_related_studies': encoded_related_studies,\n",
    "            'title': title,\n",
    "            'description': description,\n",
    "            'desired_criteria': desired_criteria,\n",
    "            'messages': messages\n",
    "        }, ignore_index=True)\n",
    "        \n",
    "        print(f\"Prompt: {messages}\")\n",
    "        print(f\"Response: {pipe(messages)}\")\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('responses_gemini.csv', index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import json\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import vertexai\n",
    "from vertexai.batch_prediction import BatchPredictionJob\n",
    "import os\n",
    "import uuid\n",
    "\n",
    "PROJECT_ID = \"PROJECT-ID\"  # update with Google Cloud project ID\n",
    "BUCKET_NAME = \"BUCKET-NAME\" # create a bucket in Google Cloud Storage first\n",
    "OUTPUT_DIR = \"./output/\"\n",
    "CSV_OUTPUT_PATH = \"./responses_gemini.csv\"\n",
    "vertexai.init(project=PROJECT_ID, location=\"us-central1\")\n",
    "\n",
    "ravis_dataset = load_dataset(\"ravistech/clinical-trial-llm-cancer-restructure\")\n",
    "\n",
    "batch_prompts = []\n",
    "info_data = {}  # key is uuid, value is the info for the prompt\n",
    "for study in tqdm(ravis_dataset['train']):\n",
    "    info_for_prompt = prompt_gen.get_info_for_prompt_gen(study)\n",
    "    \n",
    "    if info_for_prompt:\n",
    "        unique_id = str(uuid.uuid4())  # gen uuid for each entries\n",
    "        encoded_related_studies, title, description, desired_criteria = info_for_prompt\n",
    "        messages = prompt_gen.user_prompt_template(encoded_related_studies, title, description, desired_criteria)\n",
    "\n",
    "        request_format = {\n",
    "            \"id\": unique_id,\n",
    "            \"request\": {\n",
    "                \"contents\": [\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"parts\": [\n",
    "                            {\"text\": messages}\n",
    "                        ]\n",
    "                    }\n",
    "                ],\n",
    "                \"system_instruction\": {\n",
    "                    \"parts\": [\n",
    "                        {\"text\": prompt_gen.system_prompt}\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        batch_prompts.append(request_format)\n",
    "        \n",
    "        # store info with unique ID for later matching with the response\n",
    "        info_data[unique_id] = {\n",
    "            \"encoded_related_studies\": encoded_related_studies,\n",
    "            \"title\": title,\n",
    "            \"description\": description,\n",
    "            \"desired_criteria\": desired_criteria,\n",
    "            \"messages\": messages,\n",
    "            \"response\": None\n",
    "        }\n",
    "\n",
    "input_jsonl_path = f'gs://{BUCKET_NAME}/prompt_for_batch_gemini_predict.jsonl'\n",
    "with open('./prompt_for_batch_gemini_predict.jsonl', 'w') as f:\n",
    "    for prompt in batch_prompts:\n",
    "        f.write(json.dumps(prompt) + '\\n')\n",
    "\n",
    "# Upload the JSONL file to GCS\n",
    "!gsutil cp ./prompt_for_batch_gemini_predict.jsonl {input_jsonl_path}\n",
    "\n",
    "batch_prediction_job = BatchPredictionJob.submit(\n",
    "    source_model=\"gemini-1.5-flash-002\",\n",
    "    input_dataset=input_jsonl_path,\n",
    "    output_uri_prefix=f'gs://{BUCKET_NAME}/output/',\n",
    ")\n",
    "\n",
    "print(f\"Job resource name: {batch_prediction_job.resource_name}\")\n",
    "print(f\"Model resource name with the job: {batch_prediction_job.model_name}\")\n",
    "print(f\"Job state: {batch_prediction_job.state.name}\")\n",
    "\n",
    "# Check the job status\n",
    "while not batch_prediction_job.has_ended:\n",
    "    time.sleep(5)\n",
    "    batch_prediction_job.refresh()\n",
    "\n",
    "# Process the output if the job is successful\n",
    "if batch_prediction_job.has_succeeded:\n",
    "    print(\"Job succeeded!\")\n",
    "    output_location = batch_prediction_job.output_location + \"/predictions.jsonl\"\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    local_output_path = os.path.join(OUTPUT_DIR, \"predictions.jsonl\")\n",
    "\n",
    "    # Download the output file from GCS\n",
    "    !gsutil cp {output_location} {local_output_path}\n",
    "    print(f\"Output file downloaded to: {local_output_path}\")\n",
    "    \n",
    "    with open(local_output_path, 'r') as f:\n",
    "        for line in f:\n",
    "            response_data = json.loads(line)\n",
    "            unique_id = response_data.get(\"id\")\n",
    "            response = response_data.get(\"response\", {})\n",
    "            \n",
    "            if unique_id in info_data:\n",
    "                info_data[unique_id][\"response\"] = response\n",
    "\n",
    "    df = pd.DataFrame.from_dict(info_data, orient=\"index\")\n",
    "    df.to_csv(CSV_OUTPUT_PATH, index=False)\n",
    "    print(f\"Responses saved to CSV at: {CSV_OUTPUT_PATH}\")\n",
    "else:\n",
    "    print(f\"Job failed: {batch_prediction_job.error}\")\n",
    "\n",
    "print(f\"Job output location: {batch_prediction_job.output_location}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
