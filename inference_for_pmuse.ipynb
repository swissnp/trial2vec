{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from client import Client\n",
    "from prompt_gen import PromptGen\n",
    "\n",
    "client = chromadb.PersistentClient(path=\"./clinical_trials_chroma\")\n",
    "embed_model = SentenceTransformer(\"malteos/scincl\")\n",
    "collection = client.get_or_create_collection(\"clinical_trials_studies\")\n",
    "\n",
    "\n",
    "client = Client(\n",
    "  client=client,\n",
    "  collection=collection,\n",
    "  embed_model=embed_model\n",
    ")\n",
    "\n",
    "prompt_gen = PromptGen(\n",
    "  client=client\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import uuid\n",
    "\n",
    "\n",
    "ravis_dataset = load_dataset(\"ravistech/clinical-trial-llm-cancer-restructure\")\n",
    "\n",
    "\n",
    "info_data = {}  # key is uuid, value is the info for the prompt\n",
    "for study in tqdm(ravis_dataset['test']):\n",
    "\n",
    "    info_for_prompt = prompt_gen.get_info_for_prompt_gen(study)\n",
    "    \n",
    "    if info_for_prompt:\n",
    "        unique_id = str(uuid.uuid4())  # generate uuid for each entry\n",
    "        encoded_related_studies, title, description, desired_criteria = info_for_prompt\n",
    "        messages = prompt_gen.user_prompt_template(encoded_related_studies, title, description, desired_criteria)\n",
    "        \n",
    "        # store info with unique ID for later use\n",
    "        info_data[unique_id] = {\n",
    "            \"encoded_related_studies\": encoded_related_studies,\n",
    "            \"title\": title,\n",
    "            \"description\": description,\n",
    "            \"desired_criteria\": desired_criteria,\n",
    "            \"messages\": messages,\n",
    "            \"response\": None  # setting response to None\n",
    "        }\n",
    "\n",
    "df = pd.DataFrame.from_dict(info_data, orient=\"index\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Clear CUDA memory\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "impose_input_len = True\n",
    "df['input'] = df.apply(lambda x: prompt_gen.gen_input(x['encoded_related_studies'], x['title'], x['description']), axis=1)\n",
    "\n",
    "if impose_input_len:\n",
    "  from transformers import AutoTokenizer\n",
    "  tokenizer = AutoTokenizer.from_pretrained(\"neuralmagic/Meta-Llama-3.1-8B-Instruct-quantized.w8a16\")\n",
    "  df['input_len'] = df['input'].apply(lambda x: len(tokenizer(x)['input_ids']))\n",
    "  df = df[df['input_len'] < 7000]\n",
    "df[\"output\"] = \"\"\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## for the best perf will pickle the df here and load it in the next cell\n",
    "df.to_pickle(\"df_temp.pkl\")\n",
    "\n",
    "## please restart the kernel and run the following cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Restart the kernal here for the best performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load the df\n",
    "import pandas as pd\n",
    "df = pd.read_pickle(\"df_temp.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "from transformers import AutoTokenizer\n",
    "model_id = \"swissnp/finetuned_gemini_CoT_studies\"\n",
    "number_gpus = 1\n",
    "repetition_penalty = 1\n",
    "llm = LLM(model=model_id, tensor_parallel_size=number_gpus, max_model_len=12000, gpu_memory_utilization=0.93)\n",
    "def pipe(messages):\n",
    "    sampling_params = SamplingParams(temperature=0, top_p=0.9, max_tokens=4096, repetition_penalty=repetition_penalty)\n",
    "    prompts = llm.get_tokenizer().apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n",
    "    outputs = llm.generate(prompts, sampling_params)\n",
    "    print([i.outputs[0].text for i in outputs], len(outputs))\n",
    "    return [i.outputs[0].text for i in outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompt_gen import PromptGen\n",
    "\n",
    "batch_size = 20\n",
    "\n",
    "for i in range(0, len(df), batch_size):\n",
    "    print(i)\n",
    "    batch = df.iloc[i:i + batch_size]\n",
    "    batch_inputs = [PromptGen.gen_messages(row[\"input\"]) for _, row in batch.iterrows()]\n",
    "    batch_outputs = pipe(batch_inputs)\n",
    "    df.loc[batch.index, 'output'] = batch_outputs\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "\n",
    "# def extract_criteria(text):\n",
    "#     match = re.search(r\"<CRITERIA>(.*?)</CRITERIA>\", text, re.DOTALL)\n",
    "#     return match.group(1).strip() if match else None\n",
    "\n",
    "# def improved_parse_with_raw(criteria_text):\n",
    "#     criteria_text = extract_criteria(criteria_text)\n",
    "#     if not criteria_text:\n",
    "#         print(\"No criteria found\")\n",
    "#         return {}\n",
    "#     result = {\n",
    "#         \"raw_criteria\": criteria_text, \n",
    "#         \"inclusion_criteria\": [],\n",
    "#         \"exclusion_criteria\": [],\n",
    "#         \"sex\": \"ALL\",\n",
    "#         \"ages\": {\n",
    "#             \"minimum_age\": None,\n",
    "#             \"maximum_age\": None,\n",
    "#             \"age_group\": []\n",
    "#         },\n",
    "#         \"accepts_healthy_volunteers\": False\n",
    "#     }\n",
    "    \n",
    "#     inclusion_match = re.search(r\"Inclusion Criteria:(.*?)(?:Exclusion Criteria:|##|$)\", criteria_text, re.DOTALL)\n",
    "#     exclusion_match = re.search(r\"Exclusion Criteria:(.*?)(?:##|$)\", criteria_text, re.DOTALL)\n",
    "\n",
    "#     if inclusion_match:\n",
    "#         result[\"inclusion_criteria\"] = [\n",
    "#             item.strip() for item in inclusion_match.group(1).split(\"\\n\") if item.strip()\n",
    "#         ]\n",
    "#     if exclusion_match:\n",
    "#         result[\"exclusion_criteria\"] = [\n",
    "#             item.strip() for item in exclusion_match.group(1).split(\"\\n\") if item.strip()\n",
    "#         ]\n",
    "\n",
    "#     sex_match = re.search(r\"##Sex\\s*:\\s*(Male|Female|All)\", criteria_text, re.IGNORECASE)\n",
    "#     if sex_match:\n",
    "#         result[\"sex\"] = sex_match.group(1).upper()\n",
    "\n",
    "#     min_age_match = re.search(r\"- Minimum Age\\s*:\\s*(\\d+)\", criteria_text, re.IGNORECASE)\n",
    "#     if min_age_match:\n",
    "#         result[\"ages\"][\"minimum_age\"] = int(min_age_match.group(1))\n",
    "\n",
    "#     max_age_match = re.search(r\"- Maximum Age\\s*:\\s*(\\d+)\", criteria_text, re.IGNORECASE)\n",
    "#     if max_age_match:\n",
    "#         result[\"ages\"][\"maximum_age\"] = int(max_age_match.group(1))\n",
    "\n",
    "#     age_group_match = re.findall(r\"Age Group.*?:(.*?)$\", criteria_text, re.MULTILINE)\n",
    "#     if age_group_match:\n",
    "#         age_groups = re.findall(r\"(Child|Adult|Older Adult)\", \" \".join(age_group_match), re.IGNORECASE)\n",
    "#         result[\"ages\"][\"age_group\"] = list(set(group.upper() for group in age_groups))  # Unique values\n",
    "\n",
    "#     healthy_volunteers_match = re.search(r\"##Accepts Healthy Volunteers:\\s*(Yes|No)\", criteria_text, re.IGNORECASE)\n",
    "#     if healthy_volunteers_match:\n",
    "#         result[\"accepts_healthy_volunteers\"] = healthy_volunteers_match.group(1).strip().lower() == \"yes\"\n",
    "\n",
    "#     return result\n",
    "\n",
    "# df.dropna(subset=['output'], inplace=True)\n",
    "# improved_criteria_with_raw_json = df['output'].apply(improved_parse_with_raw)\n",
    "\n",
    "# df['json'] = improved_criteria_with_raw_json"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
