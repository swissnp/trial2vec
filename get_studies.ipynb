{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ravis_dataset = load_dataset(\"ravistech/clinical-trial-llm-cancer-restructure\")\n",
    "\n",
    "print(ravis_dataset)\n",
    "print(ravis_dataset['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def fix_invalid_json(input_str):\n",
    "    ## add double quotes around elements inside square brackets if not already quoted\n",
    "    fixed_str = re.sub(r'(?<=\\[)([^\\[\\],]+)(?=\\])', lambda x: '\"' + x.group(0).strip() + '\"', input_str)\n",
    "    \n",
    "    ## add double quotes around words in Conditions and Interventions\n",
    "    fixed_str = re.sub(r'(?<=\\[)([^\\\"\\]]+?)(?=\\])', lambda x: '\"' + x.group(0).strip().replace(\", \", '\", \"') + '\"', fixed_str)\n",
    "    \n",
    "    ## fix key-value pairs inside Interventions\n",
    "    fixed_str = re.sub(r'\"([A-Za-z]+): ([A-Za-z0-9\\s]+)\"', r'\"\\1: \\2\"', fixed_str)\n",
    "    \n",
    "    # fix dictionary keys\n",
    "    fixed_str = re.sub(r'(?<!\")(\\b[A-Za-z_]+\\b)(?=\\s*:)', r'\"\\1\"', fixed_str)\n",
    "    \n",
    "    return fixed_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install json_repair\n",
    "import chromadb\n",
    "from datasets import load_dataset\n",
    "import json\n",
    "import json_repair\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from unidecode import unidecode\n",
    "\n",
    "client = chromadb.PersistentClient(path=\"./clinical_trials_chroma\")\n",
    "model = SentenceTransformer(\"malteos/scincl\")\n",
    "collection = client.get_or_create_collection(\"clinical_trials_studies\")\n",
    "\n",
    "ravis_dataset = load_dataset(\"ravistech/clinical-trial-llm-cancer-restructure\")\n",
    "\n",
    "def embed_studies_from_dataset(dataset, batch_size=32):\n",
    "    batch_texts = []       \n",
    "    batch_metadata = []    \n",
    "    batch_documents = []   \n",
    "    batch_ids = []         \n",
    "    index = 1\n",
    "    length = len(dataset['train'])\n",
    "    \n",
    "    for study in dataset['train']:\n",
    "            metadata = json_repair.loads(fix_invalid_json(study['metadata']))\n",
    "            official_title = metadata.get('Official_title', '')\n",
    "            detailed_description = study.get('data', '')\n",
    "\n",
    "            # Skip if no valid officialTitle or detailedDescription\n",
    "            if not official_title or not detailed_description:\n",
    "                continue\n",
    "            concatenated_text = unidecode(f\"{official_title} [SEP] {detailed_description}\")\n",
    "            batch_texts.append(concatenated_text)\n",
    "            batch_metadata.append({\n",
    "                \"nctId\": metadata.get(\"NCT_ID\", \"unknown\"),\n",
    "                \"officialTitle\": official_title,\n",
    "                \"detailedDescription\": detailed_description,\n",
    "                \"jsonMetadata\": json.dumps(metadata, ensure_ascii=True)\n",
    "            })\n",
    "            batch_documents.append(json.dumps({\n",
    "                \"metadata\": metadata,\n",
    "                \"description\": study.get('data', ''),\n",
    "                \"criteria\": study.get('criteria', '')\n",
    "                },ensure_ascii=True))\n",
    "            batch_ids.append(metadata.get(\"NCT_ID\", \"unknown\"))\n",
    "\n",
    "            # When batch size is reached, process the batch\n",
    "            if len(batch_texts) == batch_size:\n",
    "                process_batch(batch_texts, batch_documents, batch_ids, batch_metadata)\n",
    "                print(f\"Processed {len(batch_texts)} studies. {index}/{length}\")\n",
    "                # Clear the batches\n",
    "                batch_texts.clear()\n",
    "                batch_documents.clear()\n",
    "                batch_metadata.clear()\n",
    "                batch_ids.clear()\n",
    "            index += 1\n",
    "\n",
    "    if batch_texts:\n",
    "        process_batch(batch_texts, batch_documents, batch_ids, batch_metadata)\n",
    "\n",
    "def process_batch(texts, documents, ids, metadatas):\n",
    "    embeddings = model.encode(texts, batch_size=len(texts))\n",
    "    collection.add(\n",
    "        embeddings=embeddings,\n",
    "        documents=documents,\n",
    "        metadatas=metadatas,\n",
    "        ids=ids\n",
    "    )\n",
    "    print(f\"Processed and added batch of {len(texts)} studies.\")\n",
    "\n",
    "# adjust batch_size to fit in your gpu memory\n",
    "embed_studies_from_dataset(ravis_dataset, batch_size=750)\n",
    "print(\"Embedding and storing complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
