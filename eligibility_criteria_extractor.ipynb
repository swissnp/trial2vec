{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-30 23:49:28 config.py:350] This model supports multiple tasks: {'embedding', 'generate'}. Defaulting to 'generate'.\n",
      "WARNING 12-30 23:49:28 arg_utils.py:1013] Chunked prefill is enabled by default for models with max_model_len > 32K. Currently, chunked prefill might not work with some features or models. If you encounter any issues, please disable chunked prefill by setting --enable-chunked-prefill=False.\n",
      "INFO 12-30 23:49:28 config.py:1136] Chunked prefill is enabled with max_num_batched_tokens=512.\n",
      "INFO 12-30 23:49:28 llm_engine.py:249] Initializing an LLM engine (v0.6.4.post1) with config: model='neuralmagic/Meta-Llama-3.1-8B-Instruct-quantized.w8a16', speculative_config=None, tokenizer='neuralmagic/Meta-Llama-3.1-8B-Instruct-quantized.w8a16', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=50000, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=compressed-tensors, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=neuralmagic/Meta-Llama-3.1-8B-Instruct-quantized.w8a16, num_scheduler_steps=1, chunked_prefill_enabled=True multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, chat_template_text_format=string, mm_processor_kwargs=None, pooler_config=None)\n",
      "Loading fast tokenizer from /home/swiss/.cache/huggingface/hub/models--neuralmagic--Meta-Llama-3.1-8B-Instruct-quantized.w8a16/snapshots/38e03ba250017bf8ed3eeecd3a744e21f6b994a9/tokenizer.json\n",
      "INFO 12-30 23:49:28 selector.py:135] Using Flash Attention backend.\n",
      "INFO 12-30 23:49:29 model_runner.py:1072] Starting to load model neuralmagic/Meta-Llama-3.1-8B-Instruct-quantized.w8a16...\n",
      "INFO 12-30 23:49:29 compressed_tensors_wNa16.py:83] Using MarlinLinearKernel for CompressedTensorsWNA16\n",
      "INFO 12-30 23:49:29 weight_utils.py:243] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36e35f23d56543448fe8eef3d5a19ab8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-30 23:49:32 model_runner.py:1077] Loading model weights took 8.4927 GB\n",
      "INFO 12-30 23:49:33 worker.py:232] Memory profiling results: total_gpu_memory=23.68GiB initial_memory_usage=8.97GiB peak_torch_memory=9.67GiB memory_usage_post_profile=9.00GiB non_torch_memory=0.50GiB kv_cache_size=11.15GiB gpu_memory_utilization=0.90\n",
      "INFO 12-30 23:49:33 gpu_executor.py:113] # GPU blocks: 5706, # CPU blocks: 2048\n",
      "INFO 12-30 23:49:33 gpu_executor.py:117] Maximum concurrency for 50000 tokens per request: 1.83x\n",
      "INFO 12-30 23:49:36 model_runner.py:1400] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 12-30 23:49:36 model_runner.py:1404] If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 12-30 23:49:46 model_runner.py:1518] Graph capturing finished in 10 secs, took 0.98 GiB\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "from vllm.sampling_params import GuidedDecodingParams\n",
    "from pydantic import BaseModel\n",
    "from typing import List, Optional\n",
    "from enum import Enum\n",
    "\n",
    "class SexEnum(str, Enum):\n",
    "    MALE = \"MALE\"\n",
    "    FEMALE = \"FEMALE\"\n",
    "    ALL = \"ALL\"\n",
    "\n",
    "class AgeGroupEnum(str, Enum):\n",
    "    CHILD = \"CHILD\"\n",
    "    ADULT = \"ADULT\"\n",
    "    OLDER_ADULT = \"OLDER_ADULT\"\n",
    "\n",
    "class Age(BaseModel):\n",
    "    Min: Optional[int] = None\n",
    "    Max: Optional[int] = None\n",
    "    AgeGroup: List[AgeGroupEnum]\n",
    "\n",
    "class EC(BaseModel):\n",
    "    InclusionCriteria: List[str]\n",
    "    ExclusionCriteria: List[str]\n",
    "    Sex: SexEnum\n",
    "    Age: Age\n",
    "    AcceptHealthyVolunteers: bool\n",
    "\n",
    "json_schema = EC.model_json_schema()\n",
    "model_id = \"neuralmagic/Meta-Llama-3.1-8B-Instruct-quantized.w8a16\"\n",
    "guided_decoding_params = GuidedDecodingParams(json=json_schema, backend=\"lm-format-enforcer\")\n",
    "sampling_params = SamplingParams(guided_decoding=guided_decoding_params, max_tokens=4096)\n",
    "llm = LLM(model=model_id, max_model_len=50000)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.93s/it, est. speed input: 68.37 toks/s, output: 72.13 toks/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n{\\n  \"InclusionCriteria\": [\\n    \"Diagnosis of CLL as per National Cancer Institute Working Group Guidelines\",\\n    \"Patients undergoing routine blood draws as part of their ongoing follow up for CLL\",\\n    \">= 18 years\",\\n    \"Ability to provide consent in English\",\\n    \"Patient must have measurable disease as defined by an absolute lymphocyte count greater than 5,000/mm3 or have archived lymph node or bone marrow with CLL involvement\"\\n  ],\\n  \"ExclusionCriteria\": [\\n    \"Patients who have received cytotoxic drug, oral or intravenous steroid or targeted antibody therapy for their CLL\",\\n    \"other hematologic malignancy or other disease process within the past 6 months are excluded\"\\n  ],\\n  \"Sex\": \"ALL\",\\n  \"Age\": {\\n    \"Min\": 18,\\n    \"Max\": null,\\n    \"AgeGroup\": [\\n      \"ADULT\",\\n      \"OLDER_ADULT\"\\n    ]\\n  },\\n  \"AcceptHealthyVolunteers\": false\\n} \\n  \\r\\n\\r\\n\\r\\n  '"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_prompt = \"\"\"Using the following eligibility criteria details, generate a structured JSON output that adheres to the schema provided. Do not recwrite the criteria. Just recite it in a structured JSON format.\"\"\"\n",
    "\n",
    "raw_ec = \"#Eligibility Criteria: Inclusion Criteria: * Diagnosis of CLL as per National Cancer Institute Working Group Guidelines * Patients undergoing routine blood draws as part of their ongoing follow up for CLL * >= 18 years * Ability to provide consent in English * Patient must have measurable disease as defined by an absolute lymphocyte count greater than 5,000/mm3 or have archived lymph node or bone marrow with CLL involvement. Exclusion Criteria: * Patients who have received cytotoxic drug, oral or intravenous steroid or targeted antibody therapy for their CLL, * other hematologic malignancy or other disease process within the past 6 months are excluded. ##Sex : ALL ##Ages : - Minimum Age : 18 Years - Age Group (Child: birth-17, Adult: 18-64, Older Adult: 65+) : ADULT, OLDER_ADULT ##Accepts Healthy Volunteers: No\"\n",
    "\n",
    "outputs = llm.generate(\n",
    "    prompts=\"\\n\".join([base_prompt, raw_ec]).replace(\"(Child: birth-17, Adult: 18-64, Older Adult: 65+)\", \"\"),\n",
    "    sampling_params=sampling_params,\n",
    ")\n",
    "\n",
    "print(outputs[0].outputs[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.84s/it, est. speed input: 70.31 toks/s, output: 71.37 toks/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'InclusionCriteria': ['Diagnosis of CLL as per National Cancer Institute Working Group Guidelines',\n",
       "  'Patients undergoing routine blood draws as part of their ongoing follow up for CLL',\n",
       "  '>= 18 years',\n",
       "  'Ability to provide consent in English',\n",
       "  'Patient must have measurable disease as defined by an absolute lymphocyte count greater than 5,000/mm3 or have archived lymph node or bone marrow with CLL involvement'],\n",
       " 'ExclusionCriteria': ['Patients who have received cytotoxic drug, oral or intravenous steroid or targeted antibody therapy for their CLL',\n",
       "  'Other hematologic malignancy or other disease process within the past 6 months are excluded'],\n",
       " 'Sex': 'ALL',\n",
       " 'Age': {'Min': 18, 'AgeGroup': ['ADULT', 'OLDER_ADULT']},\n",
       " 'AcceptHealthyVolunteers': False}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "def extract_ec(raw_ec: str, llm) -> EC:\n",
    "    base_prompt = \"\"\"Using the following eligibility criteria details, generate a structured JSON output that adheres to the schema provided. Do not recwrite the criteria. Just recite it in a structured JSON format.\"\"\"\n",
    "    outputs = llm.generate(\n",
    "        prompts=\"\\n\".join([base_prompt, raw_ec]).replace(\"(Child: birth-17, Adult: 18-64, Older Adult: 65+)\", \"\"),\n",
    "        sampling_params=sampling_params,\n",
    "    )\n",
    "    return_json = json.loads(outputs[0].outputs[0].text)\n",
    "    EC.model_validate(return_json)\n",
    "    return return_json\n",
    "    \n",
    "extract_ec(raw_ec, llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{\n",
      "  \"InclusionCriteria\": [\n",
      "    \"Diagnosis of CLL as per National Cancer Institute Working Group Guidelines\",\n",
      "    \"Patients undergoing routine blood draws as part of their ongoing follow up for CLL\",\n",
      "    \">= 18 years\",\n",
      "    \"Ability to provide consent in English\",\n",
      "    \"Patient must have measurable disease as defined by an absolute lymphocyte count greater than 5,000/mm3 or have archived lymph node or bone marrow with CLL involvement\"\n",
      "  ],\n",
      "  \"ExclusionCriteria\": [\n",
      "    \"Patients who have received cytotoxic drug, oral or intravenous steroid or targeted antibody therapy for their CLL\",\n",
      "    \"other hematologic malignancy or other disease process within the past 6 months are excluded\"\n",
      "  ],\n",
      "  \"Sex\": \"ALL\",\n",
      "  \"Age\": {\n",
      "    \"Min\": 18,\n",
      "    \"Max\": null,\n",
      "    \"AgeGroup\": [\n",
      "      \"ADULT\",\n",
      "      \"OLDER_ADULT\"\n",
      "    ]\n",
      "  },\n",
      "  \"AcceptHealthyVolunteers\": false\n",
      "} \n",
      "  \n",
      "\n",
      "\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "print(outputs[0].outputs[0].text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
